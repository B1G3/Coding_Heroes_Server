import logging
logger = logging.getLogger(__name__)
logger.info("llm_handler logger")

from dotenv import load_dotenv
load_dotenv()
from config import CHROMA_DB_PATH, PROMPT_PATH


from langchain.schema import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate


from core.vector_utils import get_retriever, get_vectorestore, check_chroma_db_status

chain = None

# -------------------------------------------------------- chain ìƒì„± (ì„œë²„ ì‹¤í–‰ì‹œ í•œë²ˆë§Œ í˜¸ì¶œ) ----------------------------------------------------------
def setup_chain():
    global chain
    print("=== setup_chain ë””ë²„ê¹… ì‹œì‘ ===")
    
    with open(PROMPT_PATH, "r", encoding="utf-8") as f:
        SYSTEM_PROMPT = f.read()
    print(f"âœ… System prompt ë¡œë“œ ì™„ë£Œ (ê¸¸ì´: {len(SYSTEM_PROMPT)} ë¬¸ì)")

    vectorestore = get_vectorestore()
    print(f"âœ… Chroma ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ (ê²½ë¡œ: {CHROMA_DB_PATH})")
    
    check_chroma_db_status(vectorestore)

    retriever = get_retriever(vectorestore)
    print("âœ… Retriever ìƒì„± ì™„ë£Œ")
    
    # ChatPromptTemplate ì •ì˜
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", SYSTEM_PROMPT),
        ("human", """
            ì°¸ê³  ë¬¸ì„œ:
            {context}

            ì´ì „ ëŒ€í™” ë‚´ìš©:
            {conversation_history}

            í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸: {user_question}

            ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
        """)
    ])
    print("âœ… ChatPromptTemplate ìƒì„± ì™„ë£Œ")

    # LLM ë° RAG ì²´ì¸ êµ¬ì„±
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    print("âœ… LLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def format_docs(docs):
        context_str = "\n\n".join(doc.page_content for doc in docs)
        # print("ğŸ” [RAG] context(ë¬¸ì„œ ë‚´ìš©):\n", context_str)
        return context_str


    def get_user_question(input_dict):
        return input_dict["user_question"]
    
    print("ğŸ”§ Chain êµ¬ì„± ì‹œì‘...")
    chain = (
        {
            "conversation_history": RunnablePassthrough(),
            "user_question": RunnablePassthrough(),
            "context": RunnablePassthrough() | get_user_question | retriever | format_docs,
        }
        | prompt_template
        | llm
        | StrOutputParser()
    )
    print("âœ… Chain êµ¬ì„± ì™„ë£Œ")
    return chain

# chain ì´ˆê¸°í™” í•¨ìˆ˜ - ì™¸ë¶€ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥
def initialize_chain():
    """ì™¸ë¶€ì—ì„œ chainì„ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜"""
    global chain
    if chain is None:
        chain = setup_chain()
    return chain

# chainì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
def is_chain_initialized():
    """chainì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    return chain is not None


# -------------------------------------------------------- ì§ˆì˜ì‘ë‹µ ----------------------------------------------------------
def chat(user_question: str, user_id: str = None, conversation_id: str = None) -> str:
    """
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë©”ì†Œë“œ (ì´ì „ ëŒ€í™” ë‚´ìš© ê³ ë ¤)
    
    Args:
        user_question: ì‚¬ìš©ì ì§ˆë¬¸
        user_id: ì‚¬ìš©ì ID (ì„ íƒì‚¬í•­)
        conversation_id: ëŒ€í™” ì„¸ì…˜ ID (ì„ íƒì‚¬í•­)
    """
    # chainì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ˆê¸°í™”
    if not is_chain_initialized():
        initialize_chain()
    
    # ì´ì „ ëŒ€í™” ë‚´ìš© ì¡°íšŒ
    conversation_history = ""
    if user_id and conversation_id:
        conversation_history = get_conversation_history(user_id, conversation_id)
    
    # Chainì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
    response = chain.invoke({
        "conversation_history": conversation_history,
        "user_question": user_question
    })
    
    # ëŒ€í™” ë‚´ìš© ì €ì¥ (user_idì™€ conversation_idê°€ ì œê³µëœ ê²½ìš°)
    if user_id and conversation_id:
        save_message(user_id, conversation_id, "human", user_question)
        save_message(user_id, conversation_id, "ai", response)
    
    return response



from core.database import select_messages_by_user_and_conversation_id, add_message



def get_conversation_history(user_id: str, conversation_id: str) -> str:
    """
    user_id, conv_idë¡œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ
    """
    messages = select_messages_by_user_and_conversation_id(user_id, conversation_id)

    conversation_history = []
    for msg in messages:
        role = "ì‚¬ìš©ì" if msg.role == "human" else "AI"
        conversation_history.append(f"{role}: {msg.content}")

    history = "\n".join(conversation_history)
    
    return history



def save_message(user_id: str, conversation_id: str, role: str, content: str):
    """
    ë©”ì‹œì§€ ì €ì¥
    """
    return add_message(user_id, conversation_id, role, content)


# -------------------------------------------------------- ë¸”ë¡ í”¼ë“œë°± ----------------------------------------------------------
# def analyze_blocks(json_data: dict) -> str: 
#     user_prompt = f"""
#         ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ë¸”ë¡ ì½”ë”© ê²°ê³¼ì•¼:\n
#         ```json\n
#         {json.dumps(json_data, indent=2, ensure_ascii=False)}\n
#         ```\n
#         ì´ êµ¬ì¡°ë¥¼ ë¶„ì„í•´ì„œ í”¼ë“œë°±ì„ ì¤˜
#     """
    
#     response = llm.invoke([
#         SystemMessage(content=SYSTEM_PROMPT),
#         HumanMessage(content=user_prompt)
#     ])

#     return response.content




# # claude ëª¨ë¸
# from langchain_anthropic import ChatAnthropic

# anthropic_llm = ChatAnthropic(
#     model="claude-sonnet-4-20250725",
#     temperature=0,

# )

